{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joannachang1028/95820_Application-of-NL-X-and-LLM/blob/main/95_820_A1_LSTM_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Goal & Overview\n",
        "\n",
        "**Goal.** Demonstrate the *same* NLP model—**Embedding → LSTM → Dense (logits)**—implemented in both **PyTorch** and **TensorFlow/Keras**, trained and evaluated on the **exact same inputs**, for a clean apples-to-apples comparison.\n",
        "\n",
        "**Task.** Tiny **binary sentiment** classification (1 = positive, 0 = negative) on a hand-made list of short movie-review–style sentences.\n",
        "\n",
        "**What we keep identical across PyTorch and TensorFlow/Keras**\n",
        "\n",
        "- **Data & preprocessing**\n",
        "  - Tokenization: simple whitespace; all text lowercased\n",
        "  - Vocabulary & ID mapping shared by both frameworks, with special tokens:\n",
        "    - `<pad>` → 0 (padding)\n",
        "    - `<unk>` → 1 (out-of-vocabulary)\n",
        "  - Fixed sequence length `MAX_LEN` with **right-truncation** (drop extra tokens) and **right-padding** (append `<pad>`)\n",
        "  - Deterministic train/validation split using a fixed random seed\n",
        "\n",
        "- **Model architecture**\n",
        "  - `Embedding(EMBED_DIM) → LSTM(HIDDEN_DIM) → Dense(NUM_CLASSES logits)` (no activation on the final layer; we use logits)\n",
        "\n",
        "- **Training recipe**\n",
        "  - Optimizer: **Adam** with learning rate **1e-3**\n",
        "  - Loss: **cross-entropy on logits** (`from_logits=True` in Keras; `nn.CrossEntropyLoss` in PyTorch)\n",
        "  - Epochs: small and identical in both\n",
        "  - Batch size: **full-batch** (one update per epoch) for clarity and brevity\n",
        "\n",
        "- **Inference**\n",
        "  - Same sample sentences evaluated in both implementations\n",
        "  - Report **softmax probabilities** `[P(neg), P(pos)]` and the predicted label\n",
        "\n",
        "> **Why these simplifications?**\n",
        "> We intentionally **do not use masking** and we take the **final time step** (even if it corresponds to padding) so the two frameworks behave identically for teaching. In production, you’d typically enable masking (e.g., `mask_zero=True` in Keras) or use packed sequences in PyTorch to ignore padded positions.\n",
        "\n",
        "**Takeaway.** The two implementations are functionally equivalent; any tiny performance differences come from framework-level initializers or numeric nuances, not from modeling choices.\n"
      ],
      "metadata": {
        "id": "321c7Iyig5BT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxguDgmAg0zG",
        "outputId": "28c2980c-7907-4f3e-cbc7-cc7a9f92433b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 33 | Train=9 | Val=3 | MAX_LEN=6\n",
            "First encoded example: [27 13 29 26  0  0]\n"
          ]
        }
      ],
      "source": [
        "# @title Shared setup: tiny dataset, vocab, encoding, splits\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "# --------------------------\n",
        "# Reproducibility\n",
        "# --------------------------\n",
        "SEED = 1337\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# --------------------------\n",
        "# Tiny movie-sentiment dataset: (text, label) with 1=pos, 0=neg\n",
        "# --------------------------\n",
        "data = [\n",
        "    (\"i love this movie\", 1),\n",
        "    (\"this film was great\", 1),\n",
        "    (\"amazing acting and story\", 1),\n",
        "    (\"highly recommend it\", 1),\n",
        "    (\"i hate this movie\", 0),\n",
        "    (\"this film was terrible\", 0),\n",
        "    (\"boring plot and bad acting\", 0),\n",
        "    (\"do not recommend it\", 0),\n",
        "    (\"what a fantastic experience\", 1),\n",
        "    (\"worst film ever\", 0),\n",
        "    (\"best film ever\", 1),\n",
        "    (\"what a waste of time\", 0),\n",
        "]\n",
        "\n",
        "# ============================================================\n",
        "# WORD-LEVEL VOCAB (detailed explanation)\n",
        "# ------------------------------------------------------------\n",
        "# Goal: Map each *word* to a stable integer ID that both frameworks will share.\n",
        "# Steps:\n",
        "#   1) Tokenize each sentence by whitespace (quick & deterministic).\n",
        "#   2) Count word frequencies with Counter (not required, but useful if you later want cutoffs).\n",
        "#   3) Create a vocabulary list (itos = index->token) that starts with special symbols:\n",
        "#        <pad> : used to pad short sequences to MAX_LEN (index 0 here)\n",
        "#        <unk> : used for any out-of-vocabulary word seen at inference time (index 1 here)\n",
        "#      Then append all observed words in a sorted order for determinism.\n",
        "#   4) Build the reverse map (stoi = token->index).\n",
        "#\n",
        "# Notes:\n",
        "# - We choose **PAD_ID=0** and **UNK_ID=1**, which is common and convenient.\n",
        "# - Sorting the tokens makes the mapping stable across runs (given fixed data).\n",
        "# - In more realistic pipelines, you'd lowercase consistently (we do), and maybe strip punctuation.\n",
        "# - For *strict parity* between frameworks, we’ll reuse these exact IDs everywhere.\n",
        "# ============================================================\n",
        "counter = Counter()\n",
        "for text, _ in data:\n",
        "    counter.update(text.lower().split())\n",
        "\n",
        "# Counter will look like:\n",
        "# {'this': 4, 'film': 4, 'i': 2, 'movie': 2, 'was': 2, 'acting': 2, 'and': 2,\n",
        "#  'recommend': 2, 'it': 2, 'what': 2, 'a': 2, 'ever': 2,\n",
        "#  'love': 1, 'great': 1, 'amazing': 1, 'story': 1, 'highly': 1, 'hate': 1,\n",
        "#  'terrible': 1, 'boring': 1, 'plot': 1, 'bad': 1, 'do': 1, 'not': 1,\n",
        "#  'fantastic': 1, 'experience': 1, 'worst': 1, 'best': 1,\n",
        "#  'waste': 1, 'of': 1, 'time': 1}\n",
        "\n",
        "PAD, UNK = \"<pad>\", \"<unk>\"\n",
        "itos = [PAD, UNK] + sorted(counter.keys())   # index -> token\n",
        "stoi = {w: i for i, w in enumerate(itos)}    # token -> index\n",
        "PAD_ID, UNK_ID = stoi[PAD], stoi[UNK]\n",
        "vocab_size = len(itos)\n",
        "\n",
        "# ============================================================\n",
        "# ENCODE TO FIXED LENGTH (detailed explanation)\n",
        "# ------------------------------------------------------------\n",
        "# Goal: Convert each sentence into a *fixed-length* vector of token IDs so that:\n",
        "#   - Batches can be simple NumPy arrays / tensors (no ragged shapes).\n",
        "#   - Both frameworks see identical inputs.\n",
        "#\n",
        "# Choices we make:\n",
        "#   - MAX_LEN = 6: short enough for a toy demo.\n",
        "#   - Truncation policy: keep the *first* MAX_LEN tokens (drop the rest).\n",
        "#       This is often called \"right-truncation\".\n",
        "#   - Padding policy: if a sentence has < MAX_LEN tokens, append PAD_ID to the right\n",
        "#       (\"right-padding\") until length is exactly MAX_LEN.\n",
        "#\n",
        "# Consequences:\n",
        "#   - The *last* position is often padding for short sentences. Since we take the output at\n",
        "#     the last time step, that step may correspond to <pad>. This is OK here because:\n",
        "#       * both frameworks do the exact same thing, so the comparison is fair, and\n",
        "#       * the model can learn to treat PAD as \"uninformative\".\n",
        "#   - In practice, you'd often use masking so the model ignores padded positions.\n",
        "# ============================================================\n",
        "MAX_LEN = 6\n",
        "\n",
        "def encode(text):\n",
        "    toks = text.lower().split()\n",
        "    ids = [stoi.get(tok, UNK_ID) for tok in toks]     # map tokens to IDs, OOV -> UNK_ID\n",
        "    ids = ids[:MAX_LEN]                               # truncate to MAX_LEN (right-truncation)\n",
        "    ids = ids + [PAD_ID] * (MAX_LEN - len(ids))       # right-pad with PAD_ID to fixed length\n",
        "    return ids\n",
        "\n",
        "\n",
        "# itos (index → token)\n",
        "\n",
        "# (alphabetical after the two specials)\n",
        "\n",
        "# 0: <pad>\n",
        "# 1: <unk>\n",
        "# 2: a\n",
        "# 3: acting\n",
        "# 4: amazing\n",
        "# 5: and\n",
        "# 6: bad\n",
        "# 7: best\n",
        "# 8: boring\n",
        "# 9: do\n",
        "# 10: ever\n",
        "# 11: experience\n",
        "# 12: fantastic\n",
        "# 13: film\n",
        "# 14: great\n",
        "# 15: hate\n",
        "# 16: highly\n",
        "# 17: i\n",
        "# 18: it\n",
        "# 19: love\n",
        "# 20: movie\n",
        "# 21: not\n",
        "# 22: of\n",
        "# 23: plot\n",
        "# 24: recommend\n",
        "# 25: story\n",
        "# 26: terrible\n",
        "# 27: this\n",
        "# 28: time\n",
        "# 29: was\n",
        "# 30: waste\n",
        "# 31: what\n",
        "# 32: worst\n",
        "\n",
        "# stoi (token → index)\n",
        "# {\n",
        "#  '<pad>':0, '<unk>':1,\n",
        "#  'a':2, 'acting':3, 'amazing':4, 'and':5, 'bad':6, 'best':7, 'boring':8, 'do':9,\n",
        "#  'ever':10, 'experience':11, 'fantastic':12, 'film':13, 'great':14, 'hate':15,\n",
        "#  'highly':16, 'i':17, 'it':18, 'love':19, 'movie':20, 'not':21, 'of':22, 'plot':23,\n",
        "#  'recommend':24, 'story':25, 'terrible':26, 'this':27, 'time':28, 'was':29,\n",
        "#  'waste':30, 'what':31, 'worst':32\n",
        "# }\n",
        "\n",
        "# IDs for specials and vocab size\n",
        "# PAD_ID = 0\n",
        "# UNK_ID = 1\n",
        "# vocab_size = 33\n",
        "\n",
        "# encode(t) with MAX_LEN = 6 (right-truncate then right-pad with PAD_ID)\n",
        "# encode(\"this film was great\")\n",
        "# # -> [27, 13, 29, 14, 0, 0]\n",
        "\n",
        "# encode(\"boring plot and bad acting\")\n",
        "# # -> [8, 23, 5, 6, 3, 0]\n",
        "\n",
        "# encode(\"what a fantastic experience\")\n",
        "# # -> [31, 2, 12, 11, 0, 0]\n",
        "\n",
        "# encode(\"i love this movie\")\n",
        "# # -> [17, 19, 27, 20, 0, 0]\n",
        "\n",
        "# encode(\"this film was terrible\")\n",
        "# # -> [27, 13, 29, 26, 0, 0]\n",
        "\n",
        "# encode(\"do not recommend it\")\n",
        "# # -> [9, 21, 24, 18, 0, 0]\n",
        "\n",
        "# encode(\"worst film ever\")\n",
        "# # -> [32, 13, 10, 0, 0, 0]\n",
        "\n",
        "# encode(\"best film ever\")\n",
        "# # -> [7, 13, 10, 0, 0, 0]\n",
        "\n",
        "# encode(\"what a waste of time\")\n",
        "# # -> [31, 2, 30, 22, 28, 0]\n",
        "\n",
        "# Vectorize the whole dataset\n",
        "X = np.array([encode(t) for t, _ in data], dtype=np.int32)  # int32: good for TF; will cast to long in Torch\n",
        "y = np.array([lbl for _, lbl in data], dtype=np.int32)\n",
        "\n",
        "# --------------------------\n",
        "# Train/val split (75/25), with a fixed permutation for reproducibility\n",
        "# --------------------------\n",
        "perm = np.random.RandomState(SEED).permutation(len(X))\n",
        "X, y = X[perm], y[perm]\n",
        "split = int(0.75 * len(X))\n",
        "X_train, y_train = X[:split], y[:split]\n",
        "X_val,   y_val   = X[split:], y[split:]\n",
        "\n",
        "# --------------------------\n",
        "# Shared hyperparameters (kept small for speed & clarity)\n",
        "# --------------------------\n",
        "EMBED_DIM   = 16\n",
        "HIDDEN_DIM  = 32\n",
        "NUM_CLASSES = 2\n",
        "EPOCHS = 3\n",
        "\n",
        "# We'll do *full-batch* training to keep code tiny:\n",
        "BATCH_SIZE_FULL = len(X_train)  # one update per epoch using all training examples\n",
        "\n",
        "# Three sample sentences for end-of-notebook predictions\n",
        "sample_texts = [\"this movie was great\", \"this movie was boring\", \"i recommend this film\"]\n",
        "\n",
        "def encode_batch(texts):\n",
        "    return np.array([encode(t) for t in texts], dtype=np.int32)\n",
        "\n",
        "print(f\"Vocab size: {vocab_size} | Train={len(X_train)} | Val={len(X_val)} | MAX_LEN={MAX_LEN}\")\n",
        "print(\"First encoded example:\", X_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PyTorch LSTM (tiny, full-batch)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Convert numpy arrays → torch tensors\n",
        "Xtr = torch.from_numpy(X_train).long()   # Embedding expects torch.long (int64)\n",
        "ytr = torch.from_numpy(y_train).long()\n",
        "Xva = torch.from_numpy(X_val).long()\n",
        "yva = torch.from_numpy(y_val).long()\n",
        "\n",
        "# --------------------------\n",
        "# Model: Embedding → LSTM → Dense(logits)\n",
        "# --------------------------\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, pad_id):\n",
        "        super().__init__()\n",
        "        # padding_idx=pad_id keeps the PAD row frozen at zeros (not updated by training).\n",
        "        # Note: In TF below we DON'T freeze PAD; the PAD embedding will be trainable there.\n",
        "        # That tiny discrepancy is usually negligible for this demo. If you want exact parity,\n",
        "        # remove padding_idx here so PAD is also trainable in PyTorch.\n",
        "        self.emb  = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc   = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)            # shape: (B, T, E)\n",
        "        out, _ = self.lstm(x)      # shape: (B, T, H); we ignore hidden state tuple for simplicity\n",
        "        last = out[:, -1, :]       # take the *final* time step (could be PAD; same choice in TF)\n",
        "        return self.fc(last)       # logits (unnormalized scores), shape: (B, C)\n",
        "\n",
        "model = LSTMClassifier(vocab_size, EMBED_DIM, HIDDEN_DIM, NUM_CLASSES, PAD_ID)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# ============================================================\n",
        "# FULL-BATCH GRADIENT DESCENT (detailed explanation)\n",
        "# ------------------------------------------------------------\n",
        "# \"Full-batch\" means we use *all* training examples in one giant batch to compute:\n",
        "#   1) A single forward pass producing logits for the entire training set.\n",
        "#   2) A single scalar loss value (average over all examples).\n",
        "#   3) A single backward pass computing gradients w.r.t. all model parameters.\n",
        "#   4) One optimizer step that updates parameters using those gradients.\n",
        "#\n",
        "# Why do this here?\n",
        "#   - Our dataset is tiny, so it's easy and compact to express.\n",
        "#   - It reduces code (no DataLoader or loops over mini-batches).\n",
        "#   - It ensures both frameworks take an equally simple path.\n",
        "#\n",
        "# Trade-offs (for real training):\n",
        "#   - Full-batch can be slow/ memory-heavy for large datasets.\n",
        "#   - Mini-batches add stochasticity that can help generalization.\n",
        "# ============================================================\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    opt.zero_grad()                 # clear old gradients\n",
        "    logits = model(Xtr)             # forward pass on *all* training examples\n",
        "    loss = loss_fn(logits, ytr)     # compute average cross-entropy loss over the batch\n",
        "    loss.backward()                 # backprop: compute gradients dLoss/dParam\n",
        "    opt.step()                      # update parameters (Adam optimizer)\n",
        "\n",
        "    # Quick validation accuracy (no gradient tracking)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_logits = model(Xva)\n",
        "        val_pred = val_logits.argmax(1)\n",
        "        val_acc = (val_pred == yva).float().mean().item()\n",
        "    print(f\"[PyTorch] Epoch {epoch}/{EPOCHS}  train_loss={loss.item():.4f}  val_acc={val_acc:.3f}\")\n",
        "\n",
        "# Inference on sample sentences\n",
        "with torch.no_grad():\n",
        "    logits = model(torch.from_numpy(encode_batch(sample_texts)).long())\n",
        "    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "    preds = probs.argmax(1)\n",
        "\n",
        "print(\"\\n[PyTorch] Predictions:\")\n",
        "for t, p, pr in zip(sample_texts, preds, probs):\n",
        "    print(f\"{t!r} → {'pos' if p==1 else 'neg'}  probs={pr}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxR4mqbh1cj3",
        "outputId": "ff069646-3cf4-4abb-c47a-9f8c5847921c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PyTorch] Epoch 1/3  train_loss=0.6908  val_acc=0.333\n",
            "[PyTorch] Epoch 2/3  train_loss=0.6891  val_acc=0.333\n",
            "[PyTorch] Epoch 3/3  train_loss=0.6874  val_acc=0.333\n",
            "\n",
            "[PyTorch] Predictions:\n",
            "'this movie was great' → pos  probs=[0.4464912 0.5535088]\n",
            "'this movie was boring' → pos  probs=[0.4553566  0.54464334]\n",
            "'i recommend this film' → pos  probs=[0.43842876 0.56157124]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TensorFlow/Keras LSTM (tiny, full-batch)\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Same architecture: Embedding → LSTM → Dense(logits)\n",
        "tf_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size,\n",
        "                              output_dim=EMBED_DIM,\n",
        "                              input_length=MAX_LEN),\n",
        "    tf.keras.layers.LSTM(HIDDEN_DIM),\n",
        "    tf.keras.layers.Dense(NUM_CLASSES)  # logits (no activation)\n",
        "])\n",
        "\n",
        "tf_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# FULL-BATCH GRADIENT DESCENT IN KERAS (what happens under the hood)\n",
        "# ------------------------------------------------------------\n",
        "# We set batch_size = number of training examples → one update per epoch.\n",
        "# Keras still handles the same steps internally:\n",
        "#   forward → loss → backward (autodiff) → optimizer step → repeat per epoch\n",
        "# This mirrors the PyTorch loop conceptually, just handled by .fit().\n",
        "# ============================================================\n",
        "tf_model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=len(X_train),   # full-batch (one gradient update per epoch)\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(X_val, y_val),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Inference on sample sentences (same as PyTorch)\n",
        "arr = encode_batch(sample_texts)\n",
        "logits = tf_model(arr, training=False)\n",
        "probs = tf.nn.softmax(logits, axis=1).numpy()\n",
        "preds = probs.argmax(1)\n",
        "\n",
        "print(\"\\n[TensorFlow] Predictions:\")\n",
        "for t, p, pr in zip(sample_texts, preds, probs):\n",
        "    print(f\"{t!r} → {'pos' if p==1 else 'neg'}  probs={pr}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaTNRVxh1jfx",
        "outputId": "be1a8ba7-2c15-416c-d05f-dce7efb0c437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.6667 - loss: 0.6902 - val_accuracy: 0.3333 - val_loss: 0.6976\n",
            "Epoch 2/3\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5556 - loss: 0.6891 - val_accuracy: 0.3333 - val_loss: 0.6997\n",
            "Epoch 3/3\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5556 - loss: 0.6880 - val_accuracy: 0.3333 - val_loss: 0.7020\n",
            "\n",
            "[TensorFlow] Predictions:\n",
            "'this movie was great' → pos  probs=[0.48581555 0.5141845 ]\n",
            "'this movie was boring' → pos  probs=[0.486884 0.513116]\n",
            "'i recommend this film' → pos  probs=[0.48593917 0.51406074]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ASNHahyc1twk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}