{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joannachang1028/95820_Application-of-NL-X-and-LLM/blob/main/95_820_A1_LSTM_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Goal & Overview\n",
        "\n",
        "**Goal.** Demonstrate the *same* NLP model—**Embedding → LSTM → Dense (logits)**—implemented in both **PyTorch** and **TensorFlow/Keras**, trained and evaluated on the **exact same inputs**, for a clean apples-to-apples comparison.\n",
        "\n",
        "**Task.** Tiny **binary sentiment** classification (1 = positive, 0 = negative) on a hand-made list of short movie-review–style sentences.\n",
        "\n",
        "**What we keep identical across PyTorch and TensorFlow/Keras**\n",
        "\n",
        "- **Data & preprocessing**\n",
        "  - Tokenization: simple whitespace; all text lowercased\n",
        "  - Vocabulary & ID mapping shared by both frameworks, with special tokens:\n",
        "    - `<pad>` → 0 (padding)\n",
        "    - `<unk>` → 1 (out-of-vocabulary)\n",
        "  - Fixed sequence length `MAX_LEN` with **right-truncation** (drop extra tokens) and **right-padding** (append `<pad>`)\n",
        "  - Deterministic train/validation split using a fixed random seed\n",
        "\n",
        "- **Model architecture**\n",
        "  - `Embedding(EMBED_DIM) → LSTM(HIDDEN_DIM) → Dense(NUM_CLASSES logits)` (no activation on the final layer; we use logits)\n",
        "\n",
        "- **Training recipe**\n",
        "  - Optimizer: **Adam** with learning rate **1e-3**\n",
        "  - Loss: **cross-entropy on logits** (`from_logits=True` in Keras; `nn.CrossEntropyLoss` in PyTorch)\n",
        "  - Epochs: small and identical in both\n",
        "  - Batch size: **full-batch** (one update per epoch) for clarity and brevity\n",
        "\n",
        "- **Inference**\n",
        "  - Same sample sentences evaluated in both implementations\n",
        "  - Report **softmax probabilities** `[P(neg), P(pos)]` and the predicted label\n",
        "\n",
        "> **Why these simplifications?**\n",
        "> We intentionally **do not use masking** and we take the **final time step** (even if it corresponds to padding) so the two frameworks behave identically for teaching. In production, you’d typically enable masking (e.g., `mask_zero=True` in Keras) or use packed sequences in PyTorch to ignore padded positions.\n",
        "\n",
        "**Takeaway.** The two implementations are functionally equivalent; any tiny performance differences come from framework-level initializers or numeric nuances, not from modeling choices.\n"
      ],
      "metadata": {
        "id": "321c7Iyig5BT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 詳細解釋「訓練設置」\n",
        "訓練設置是指在訓練機器學習模型時所使用的一系列策略和參數。這包括如何計算模型的錯誤（損失函數），如何調整模型的內部參數以最小化錯誤（優化器和學習率），以及數據如何被饋送到模型中進行訓練（epoch 和 batch size）。在這個筆記本中，訓練設置的目的是為了讓 PyTorch 和 TensorFlow 模型能夠在相同的條件下學習，以便進行公平的比較。\n",
        "\n",
        "* 優化器 (Optimizer):\n",
        "優化器是一種算法，用於根據損失函數計算出的梯度來更新模型的權重。梯度指示了損失函數在每個權重上的變化方向和大小，優化器利用這些信息來決定如何調整權重，以便逐步減小損失。\n",
        "Adam (Adaptive Moment Estimation) 是一種非常流行的優化器。它結合了兩種其他優化器 (RMSprop 和 AdaGrad) 的優點，能夠有效地處理稀疏梯度和非穩定的目標函數。Adam 會為模型的每個參數獨立地調整學習率，通常能夠比傳統的隨機梯度下降 (SGD) 更快地收斂。\n",
        "\n",
        "* 學習率 (Learning Rate):\n",
        "學習率是一個超參數，它決定了優化器在每次更新權重時邁出的步長大小。\n",
        "1e-3 (0.001) 是一個常用的預設學習率。對於不同的任務和模型，最佳學習率可能會有所不同。\n",
        "學習率的設定會影響訓練的速度和結果：\n",
        "學習率太高：模型可能會在損失函數的最小值周圍震盪，甚至發散，導致訓練不穩定或失敗。\n",
        "學習率太低：模型會非常緩慢地學習，需要更多的 epoch 才能收斂，可能會陷入局部最小值。\n",
        "這樣是快還是慢？ 相較於一些非常小的學習率（如 1e-5），1e-3 算是中等偏快的學習率。對於這個小型數據集，這個學習率是合理的，可以較快地進行實驗。\n",
        "\n",
        "* 損失函數 (Loss Function):\n",
        "損失函數用於衡量模型預測值與真實標籤之間的差異。訓練的目標是最小化損失函數的值。\n",
        "交叉熵損失 (Cross-Entropy Loss)：這是用於分類問題的一種常用損失函數。它衡量了模型輸出概率分佈與真實標籤概率分佈之間的差異。\n",
        "在這個二元分類任務中，模型輸出的是兩個類別的 logits (未經過激活函數的原始分數)。tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) (TensorFlow) 和 nn.CrossEntropyLoss() (PyTorch) 都會自動處理 logits，並計算出對應的交叉熵損失。\n",
        "from_logits=True 的設置告訴損失函數模型的輸出是 logits，而不是已經經過 softmax 處理的概率。損失函數會在內部應用 softmax 來計算損失，這在數值上更穩定。\n",
        "Epoch 數值 (Epochs):\n",
        "一個 epoch 表示整個訓練數據集被模型完整地遍歷並用於參數更新一次。\n",
        "在這個筆記本中，EPOCHS = 3 表示模型會對整個訓練數據集進行 3 次完整的訓練。\n",
        "對於小型數據集和演示目的，少量的 epoch 數是可以接受的，可以快速展示模型的訓練過程。在實際應用中，通常需要更多的 epoch 才能讓模型充分學習。\n",
        "Batch Size:\n",
        "Batch Size 決定了在每次參數更新時使用多少個訓練樣本。\n",
        "\n",
        "* 全批量 (Full-Batch)：在這個筆記本中，BATCH_SIZE_FULL = len(X_train)，這意味著每次更新權重時都使用了所有的訓練樣本。這也是「全批量梯度下降」的含義。\n",
        "優點：對於小型數據集，全批量訓練可以提供更精確的梯度估計。\n",
        "缺點：對於大型數據集，全批量訓練會非常耗費內存和計算資源，並且可能收斂較慢。在實際應用中，更常用的是 mini-batch (小批量) 訓練，它在效率和梯度估計的準確性之間取得了平衡。\n",
        "\n",
        "* Softmax 概率\n",
        "Softmax 函數 是一種常用的激活函數，通常用於多類別分類問題的輸出層之後。它將模型的原始輸出 (logits) 轉換為一個概率分佈，使得所有類別的輸出值介於 0 和 1 之間，並且所有輸出值的總和為 1。\n",
        "在這個二元分類任務中，softmax 函數將模型的兩個 logits 輸出轉換為兩個概率值，分別表示屬於負面類別和正面類別的概率 [P(neg), P(pos)]。例如，如果模型的 logits 是 [1.0, 2.0]，經過 softmax 處理後可能得到 [0.2689, 0.7311]，這表示模型預測是負面的概率約為 26.89%，是正面的概率約為 73.11%。\n",
        "* Softmax 概率是一種衡量 binary classification 的 metrics 嗎？ Softmax 概率本身不是衡量二元分類整體性能的 metrics。它是模型對於單個樣本的預測輸出，表示模型對該樣本屬於每個類別的置信度。\n",
        "衡量二元分類性能的常用 metrics 包括：準確率 (Accuracy)、精確率 (Precision)、召回率 (Recall)、F1 分數、AUC (Area Under the ROC Curve) 等。\n",
        "你可以根據 softmax 概率來決定最終的預測類別 (通常選擇概率最高的那一個)，然後使用真實標籤來計算上述的性能 metrics。"
      ],
      "metadata": {
        "id": "LCdIytHE6leL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 為何刻意不使用 masking?\n",
        "* 什麼是 Masking?\n",
        "在處理變長序列（如句子）時，我們經常需要將它們填充到相同的長度 (MAX_LEN)，以便能夠將多個序列組成一個批次 (batch) 進行處理。填充的部分通常是特殊的 padding tokens (例如 <pad>)。\n",
        "Masking 的目的是告訴模型哪些位置是填充的，以便模型在計算時忽略這些填充位置的信息。例如，在計算平均值或注意力分數時，不應該考慮填充位置的值。\n",
        "為什麼在這個範例中不使用 Masking?\n",
        "PyTorch 和 TensorFlow/Keras 在處理 masking 的方式上存在一些細微的差異。雖然它們都提供了 masking 的功能，但底層的實現和預設行為可能不同。\n",
        "\n",
        "為了避免這些框架層級的差異影響模型的比較結果，這個範例刻意選擇不使用 masking。這意味著模型在處理序列時，會將填充位置 (<pad>) 也視為有效的輸入，並且它們的 embedding 和在 LSTM 中的計算都會被考慮進去。\n",
        "\n",
        "* 這樣做的影響：\n",
        "模型需要學習如何處理填充位置。理想情況下，模型會學習到 <pad> 的 embedding 是不包含有用信息的，並且 LSTM 在處理到填充位置時，其狀態不會發生顯著變化。\n",
        "\n",
        "在實際應用中，通常會使用 masking，因為它可以提高模型的效率 (不計算填充位置) 和性能 (模型不會被填充信息干擾)。但在這個比較範例中，犧牲一點性能來換取框架間行為的完全一致是值得的。\n",
        "採取 final time step 為何可以讓兩種演算法 (PyTorch 和 TensorFlow) 達到相同的效果?\n",
        "\n",
        "* LSTM 的輸出：\n",
        "LSTM 層通常會輸出兩個東西：\n",
        "所有時間步的隱藏狀態 (hidden states)：形狀通常是 (batch_size, sequence_length, hidden_dim)。這包含了 LSTM 在處理序列中每個詞元後的內部狀態。\n",
        "最後一個時間步的隱藏狀態和單元狀態 (final hidden and cell states)：形狀通常是 (num_layers * num_directions, batch_size, hidden_dim)。這代表了 LSTM 處理完整個序列後的最終狀態。\n",
        "為什麼選擇「最後一個時間步的所有時間步隱藏狀態」?\n",
        "在這個範例中，模型不是直接使用 LSTM 的最終隱藏狀態元組 (h_n, c_n)，而是使用了「所有時間步隱藏狀態」矩陣的「最後一個時間步」的輸出。\n",
        "在 PyTorch 中，這對應於 out[:, -1, :]。\n",
        "在 TensorFlow/Keras 中，當 return_sequences=False (預設值) 時，LSTM 層直接輸出每個樣本的最後一個時間步的隱藏狀態。\n",
        "如何確保相同效果?\n",
        "由於序列都被填充到了相同的 MAX_LEN，每個序列的「最後一個時間步」都對應於索引 MAX_LEN - 1。\n",
        "無論這個時間步對應的是序列的真實結尾詞元，還是填充詞元 <pad>，兩個框架都會從 LSTM 輸出矩陣的相同索引位置提取出隱藏狀態。\n",
        "關鍵在於： 由於我們沒有使用 masking，LSTM 在處理 <pad> 時的計算是標準的 LSTM 前向傳播，其權重和計算方式在 PyTorch 和 TensorFlow 中是等效的 (給定相同的輸入和初始權重)。因此，無論最後一個時間步是真實詞元還是 <pad>，兩個框架都會根據相同的計算邏輯得出這個時間步的隱藏狀態。\n",
        "如果使用了 masking，框架在處理填充位置時的行為會有所不同，這就會導致最後一個有效時間步的選擇和計算方式不同，從而影響模型的行為。\n",
        "\n",
        "總結來說，簡化的原因是為了：\n",
        "\n",
        "1. 避免 PyTorch 和 TensorFlow/Keras 在處理變長序列和 masking 機制上的潛在差異。\n",
        "2. 確保兩個框架在處理填充序列時，在完全相同的位置 (即固定的 MAX_LEN - 1 索引) 獲取 LSTM 的輸出。\n",
        "3. 使得兩個模型的輸入、模型結構的每個層的計算過程（包括處理填充位置）以及如何從 LSTM 獲取最終表示在邏輯上完全一致，從而實現真正意義上的「蘋果對蘋果」比較。\n",
        "\n",
        "這樣做的犧牲是，模型可能需要學習如何忽略填充位置的信息，而且在處理變長序列時不如使用 masking 的模型效率高。但對於比較框架本身而言，這是一種有效的控制變量的方法。"
      ],
      "metadata": {
        "id": "awxGvmoM7bf2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxguDgmAg0zG",
        "outputId": "7f529034-f092-41e2-e3fc-3b7bb9f46165"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 33 | Train=9 | Val=3 | MAX_LEN=6\n",
            "First encoded example: [27 13 29 26  0  0]\n"
          ]
        }
      ],
      "source": [
        "# @title Shared setup: tiny dataset, vocab, encoding, splits\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "# --------------------------\n",
        "# Reproducibility\n",
        "# --------------------------\n",
        "SEED = 1337\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# --------------------------\n",
        "# Tiny movie-sentiment dataset: (text, label) with 1=pos, 0=neg\n",
        "# --------------------------\n",
        "data = [\n",
        "    (\"i love this movie\", 1),\n",
        "    (\"this film was great\", 1),\n",
        "    (\"amazing acting and story\", 1),\n",
        "    (\"highly recommend it\", 1),\n",
        "    (\"i hate this movie\", 0),\n",
        "    (\"this film was terrible\", 0),\n",
        "    (\"boring plot and bad acting\", 0),\n",
        "    (\"do not recommend it\", 0),\n",
        "    (\"what a fantastic experience\", 1),\n",
        "    (\"worst film ever\", 0),\n",
        "    (\"best film ever\", 1),\n",
        "    (\"what a waste of time\", 0),\n",
        "]\n",
        "\n",
        "# ============================================================\n",
        "# WORD-LEVEL VOCAB (detailed explanation)\n",
        "# ------------------------------------------------------------\n",
        "# Goal: Map each *word* to a stable integer ID that both frameworks will share.\n",
        "# Steps:\n",
        "#   1) Tokenize each sentence by whitespace (quick & deterministic).\n",
        "#   2) Count word frequencies with Counter (not required, but useful if you later want cutoffs).\n",
        "#   3) Create a vocabulary list (itos = index->token) that starts with special symbols:\n",
        "#        <pad> : used to pad short sequences to MAX_LEN (index 0 here)\n",
        "#        <unk> : used for any out-of-vocabulary word seen at inference time (index 1 here)\n",
        "#      Then append all observed words in a sorted order for determinism.\n",
        "#   4) Build the reverse map (stoi = token->index).\n",
        "#\n",
        "# Notes:\n",
        "# - We choose **PAD_ID=0** and **UNK_ID=1**, which is common and convenient.\n",
        "# - Sorting the tokens makes the mapping stable across runs (given fixed data).\n",
        "# - In more realistic pipelines, you'd lowercase consistently (we do), and maybe strip punctuation.\n",
        "# - For *strict parity* between frameworks, we’ll reuse these exact IDs everywhere.\n",
        "# ============================================================\n",
        "counter = Counter()\n",
        "for text, _ in data:\n",
        "    counter.update(text.lower().split())\n",
        "\n",
        "# Counter will look like:\n",
        "# {'this': 4, 'film': 4, 'i': 2, 'movie': 2, 'was': 2, 'acting': 2, 'and': 2,\n",
        "#  'recommend': 2, 'it': 2, 'what': 2, 'a': 2, 'ever': 2,\n",
        "#  'love': 1, 'great': 1, 'amazing': 1, 'story': 1, 'highly': 1, 'hate': 1,\n",
        "#  'terrible': 1, 'boring': 1, 'plot': 1, 'bad': 1, 'do': 1, 'not': 1,\n",
        "#  'fantastic': 1, 'experience': 1, 'worst': 1, 'best': 1,\n",
        "#  'waste': 1, 'of': 1, 'time': 1}\n",
        "\n",
        "PAD, UNK = \"<pad>\", \"<unk>\"\n",
        "itos = [PAD, UNK] + sorted(counter.keys())   # index -> token\n",
        "stoi = {w: i for i, w in enumerate(itos)}    # token -> index\n",
        "PAD_ID, UNK_ID = stoi[PAD], stoi[UNK]\n",
        "vocab_size = len(itos)\n",
        "\n",
        "# ============================================================\n",
        "# ENCODE TO FIXED LENGTH (detailed explanation)\n",
        "# ------------------------------------------------------------\n",
        "# Goal: Convert each sentence into a *fixed-length* vector of token IDs so that:\n",
        "#   - Batches can be simple NumPy arrays / tensors (no ragged shapes).\n",
        "#   - Both frameworks see identical inputs.\n",
        "#\n",
        "# Choices we make:\n",
        "#   - MAX_LEN = 6: short enough for a toy demo.\n",
        "#   - Truncation policy: keep the *first* MAX_LEN tokens (drop the rest).\n",
        "#       This is often called \"right-truncation\".\n",
        "#   - Padding policy: if a sentence has < MAX_LEN tokens, append PAD_ID to the right\n",
        "#       (\"right-padding\") until length is exactly MAX_LEN.\n",
        "#\n",
        "# Consequences:\n",
        "#   - The *last* position is often padding for short sentences. Since we take the output at\n",
        "#     the last time step, that step may correspond to <pad>. This is OK here because:\n",
        "#       * both frameworks do the exact same thing, so the comparison is fair, and\n",
        "#       * the model can learn to treat PAD as \"uninformative\".\n",
        "#   - In practice, you'd often use masking so the model ignores padded positions.\n",
        "# ============================================================\n",
        "MAX_LEN = 6\n",
        "\n",
        "def encode(text):\n",
        "    toks = text.lower().split()\n",
        "    ids = [stoi.get(tok, UNK_ID) for tok in toks]     # map tokens to IDs, OOV -> UNK_ID\n",
        "    ids = ids[:MAX_LEN]                               # truncate to MAX_LEN (right-truncation)\n",
        "    ids = ids + [PAD_ID] * (MAX_LEN - len(ids))       # right-pad with PAD_ID to fixed length\n",
        "    return ids\n",
        "\n",
        "\n",
        "# itos (index → token)\n",
        "\n",
        "# (alphabetical after the two specials)\n",
        "\n",
        "# 0: <pad>\n",
        "# 1: <unk>\n",
        "# 2: a\n",
        "# 3: acting\n",
        "# 4: amazing\n",
        "# 5: and\n",
        "# 6: bad\n",
        "# 7: best\n",
        "# 8: boring\n",
        "# 9: do\n",
        "# 10: ever\n",
        "# 11: experience\n",
        "# 12: fantastic\n",
        "# 13: film\n",
        "# 14: great\n",
        "# 15: hate\n",
        "# 16: highly\n",
        "# 17: i\n",
        "# 18: it\n",
        "# 19: love\n",
        "# 20: movie\n",
        "# 21: not\n",
        "# 22: of\n",
        "# 23: plot\n",
        "# 24: recommend\n",
        "# 25: story\n",
        "# 26: terrible\n",
        "# 27: this\n",
        "# 28: time\n",
        "# 29: was\n",
        "# 30: waste\n",
        "# 31: what\n",
        "# 32: worst\n",
        "\n",
        "# stoi (token → index)\n",
        "# {\n",
        "#  '<pad>':0, '<unk>':1,\n",
        "#  'a':2, 'acting':3, 'amazing':4, 'and':5, 'bad':6, 'best':7, 'boring':8, 'do':9,\n",
        "#  'ever':10, 'experience':11, 'fantastic':12, 'film':13, 'great':14, 'hate':15,\n",
        "#  'highly':16, 'i':17, 'it':18, 'love':19, 'movie':20, 'not':21, 'of':22, 'plot':23,\n",
        "#  'recommend':24, 'story':25, 'terrible':26, 'this':27, 'time':28, 'was':29,\n",
        "#  'waste':30, 'what':31, 'worst':32\n",
        "# }\n",
        "\n",
        "# IDs for specials and vocab size\n",
        "# PAD_ID = 0\n",
        "# UNK_ID = 1\n",
        "# vocab_size = 33\n",
        "\n",
        "# encode(t) with MAX_LEN = 6 (right-truncate then right-pad with PAD_ID)\n",
        "# encode(\"this film was great\")\n",
        "# # -> [27, 13, 29, 14, 0, 0]\n",
        "\n",
        "# encode(\"boring plot and bad acting\")\n",
        "# # -> [8, 23, 5, 6, 3, 0]\n",
        "\n",
        "# encode(\"what a fantastic experience\")\n",
        "# # -> [31, 2, 12, 11, 0, 0]\n",
        "\n",
        "# encode(\"i love this movie\")\n",
        "# # -> [17, 19, 27, 20, 0, 0]\n",
        "\n",
        "# encode(\"this film was terrible\")\n",
        "# # -> [27, 13, 29, 26, 0, 0]\n",
        "\n",
        "# encode(\"do not recommend it\")\n",
        "# # -> [9, 21, 24, 18, 0, 0]\n",
        "\n",
        "# encode(\"worst film ever\")\n",
        "# # -> [32, 13, 10, 0, 0, 0]\n",
        "\n",
        "# encode(\"best film ever\")\n",
        "# # -> [7, 13, 10, 0, 0, 0]\n",
        "\n",
        "# encode(\"what a waste of time\")\n",
        "# # -> [31, 2, 30, 22, 28, 0]\n",
        "\n",
        "# Vectorize the whole dataset\n",
        "X = np.array([encode(t) for t, _ in data], dtype=np.int32)  # int32: good for TF; will cast to long in Torch\n",
        "y = np.array([lbl for _, lbl in data], dtype=np.int32)\n",
        "\n",
        "# --------------------------\n",
        "# Train/val split (75/25), with a fixed permutation for reproducibility\n",
        "# --------------------------\n",
        "perm = np.random.RandomState(SEED).permutation(len(X))\n",
        "X, y = X[perm], y[perm]\n",
        "split = int(0.75 * len(X))\n",
        "X_train, y_train = X[:split], y[:split]\n",
        "X_val,   y_val   = X[split:], y[split:]\n",
        "\n",
        "# --------------------------\n",
        "# Shared hyperparameters (kept small for speed & clarity)\n",
        "# --------------------------\n",
        "EMBED_DIM   = 16\n",
        "HIDDEN_DIM  = 32\n",
        "NUM_CLASSES = 2\n",
        "EPOCHS = 3\n",
        "\n",
        "# We'll do *full-batch* training to keep code tiny:\n",
        "BATCH_SIZE_FULL = len(X_train)  # one update per epoch using all training examples\n",
        "\n",
        "# Three sample sentences for end-of-notebook predictions\n",
        "sample_texts = [\"this movie was great\", \"this movie was boring\", \"i recommend this film\"]\n",
        "\n",
        "def encode_batch(texts):\n",
        "    return np.array([encode(t) for t in texts], dtype=np.int32)\n",
        "\n",
        "print(f\"Vocab size: {vocab_size} | Train={len(X_train)} | Val={len(X_val)} | MAX_LEN={MAX_LEN}\")\n",
        "print(\"First encoded example:\", X_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PyTorch LSTM (tiny, full-batch)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Convert numpy arrays → torch tensors\n",
        "Xtr = torch.from_numpy(X_train).long()   # Embedding expects torch.long (int64)\n",
        "ytr = torch.from_numpy(y_train).long()\n",
        "Xva = torch.from_numpy(X_val).long()\n",
        "yva = torch.from_numpy(y_val).long()\n",
        "\n",
        "# --------------------------\n",
        "# Model: Embedding → LSTM → Dense(logits)\n",
        "# --------------------------\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, pad_id):\n",
        "        super().__init__()\n",
        "        # padding_idx=pad_id keeps the PAD row frozen at zeros (not updated by training).\n",
        "        # Note: In TF below we DON'T freeze PAD; the PAD embedding will be trainable there.\n",
        "        # That tiny discrepancy is usually negligible for this demo. If you want exact parity,\n",
        "        # remove padding_idx here so PAD is also trainable in PyTorch.\n",
        "        self.emb  = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc   = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)            # shape: (B, T, E)\n",
        "        out, _ = self.lstm(x)      # shape: (B, T, H); we ignore hidden state tuple for simplicity\n",
        "        last = out[:, -1, :]       # take the *final* time step (could be PAD; same choice in TF)\n",
        "        return self.fc(last)       # logits (unnormalized scores), shape: (B, C)\n",
        "\n",
        "model = LSTMClassifier(vocab_size, EMBED_DIM, HIDDEN_DIM, NUM_CLASSES, PAD_ID)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# ============================================================\n",
        "# FULL-BATCH GRADIENT DESCENT (detailed explanation)\n",
        "# ------------------------------------------------------------\n",
        "# \"Full-batch\" means we use *all* training examples in one giant batch to compute:\n",
        "#   1) A single forward pass producing logits for the entire training set.\n",
        "#   2) A single scalar loss value (average over all examples).\n",
        "#   3) A single backward pass computing gradients w.r.t. all model parameters.\n",
        "#   4) One optimizer step that updates parameters using those gradients.\n",
        "#\n",
        "# Why do this here?\n",
        "#   - Our dataset is tiny, so it's easy and compact to express.\n",
        "#   - It reduces code (no DataLoader or loops over mini-batches).\n",
        "#   - It ensures both frameworks take an equally simple path.\n",
        "#\n",
        "# Trade-offs (for real training):\n",
        "#   - Full-batch can be slow/ memory-heavy for large datasets.\n",
        "#   - Mini-batches add stochasticity that can help generalization.\n",
        "# ============================================================\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    opt.zero_grad()                 # clear old gradients\n",
        "    logits = model(Xtr)             # forward pass on *all* training examples\n",
        "    loss = loss_fn(logits, ytr)     # compute average cross-entropy loss over the batch\n",
        "    loss.backward()                 # backprop: compute gradients dLoss/dParam\n",
        "    opt.step()                      # update parameters (Adam optimizer)\n",
        "\n",
        "    # Quick validation accuracy (no gradient tracking)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_logits = model(Xva)\n",
        "        val_pred = val_logits.argmax(1)\n",
        "        val_acc = (val_pred == yva).float().mean().item()\n",
        "    print(f\"[PyTorch] Epoch {epoch}/{EPOCHS}  train_loss={loss.item():.4f}  val_acc={val_acc:.3f}\")\n",
        "\n",
        "# Inference on sample sentences\n",
        "with torch.no_grad():\n",
        "    logits = model(torch.from_numpy(encode_batch(sample_texts)).long())\n",
        "    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "    preds = probs.argmax(1)\n",
        "\n",
        "print(\"\\n[PyTorch] Predictions:\")\n",
        "for t, p, pr in zip(sample_texts, preds, probs):\n",
        "    print(f\"{t!r} → {'pos' if p==1 else 'neg'}  probs={pr}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxR4mqbh1cj3",
        "outputId": "d9bb113b-edf5-4676-c90a-f60d5b4eb3b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PyTorch] Epoch 1/3  train_loss=0.6908  val_acc=0.333\n",
            "[PyTorch] Epoch 2/3  train_loss=0.6891  val_acc=0.333\n",
            "[PyTorch] Epoch 3/3  train_loss=0.6874  val_acc=0.333\n",
            "\n",
            "[PyTorch] Predictions:\n",
            "'this movie was great' → pos  probs=[0.4464912 0.5535088]\n",
            "'this movie was boring' → pos  probs=[0.4553566  0.54464334]\n",
            "'i recommend this film' → pos  probs=[0.43842876 0.56157124]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TensorFlow/Keras LSTM (tiny, full-batch)\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Same architecture: Embedding → LSTM → Dense(logits)\n",
        "tf_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size,\n",
        "                              output_dim=EMBED_DIM,\n",
        "                              input_length=MAX_LEN),\n",
        "    tf.keras.layers.LSTM(HIDDEN_DIM),\n",
        "    tf.keras.layers.Dense(NUM_CLASSES)  # logits (no activation)\n",
        "])\n",
        "\n",
        "tf_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# FULL-BATCH GRADIENT DESCENT IN KERAS (what happens under the hood)\n",
        "# ------------------------------------------------------------\n",
        "# We set batch_size = number of training examples → one update per epoch.\n",
        "# Keras still handles the same steps internally:\n",
        "#   forward → loss → backward (autodiff) → optimizer step → repeat per epoch\n",
        "# This mirrors the PyTorch loop conceptually, just handled by .fit().\n",
        "# ============================================================\n",
        "tf_model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=len(X_train),   # full-batch (one gradient update per epoch)\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(X_val, y_val),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Inference on sample sentences (same as PyTorch)\n",
        "arr = encode_batch(sample_texts)\n",
        "logits = tf_model(arr, training=False)\n",
        "probs = tf.nn.softmax(logits, axis=1).numpy()\n",
        "preds = probs.argmax(1)\n",
        "\n",
        "print(\"\\n[TensorFlow] Predictions:\")\n",
        "for t, p, pr in zip(sample_texts, preds, probs):\n",
        "    print(f\"{t!r} → {'pos' if p==1 else 'neg'}  probs={pr}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaTNRVxh1jfx",
        "outputId": "8affecfe-820e-4cfe-85b3-d4b2280f38c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.6667 - loss: 0.6902 - val_accuracy: 0.3333 - val_loss: 0.6976\n",
            "Epoch 2/3\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step - accuracy: 0.5556 - loss: 0.6891 - val_accuracy: 0.3333 - val_loss: 0.6997\n",
            "Epoch 3/3\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.5556 - loss: 0.6880 - val_accuracy: 0.3333 - val_loss: 0.7020\n",
            "\n",
            "[TensorFlow] Predictions:\n",
            "'this movie was great' → pos  probs=[0.48581555 0.5141845 ]\n",
            "'this movie was boring' → pos  probs=[0.486884 0.513116]\n",
            "'i recommend this film' → pos  probs=[0.48593917 0.51406074]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ASNHahyc1twk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}